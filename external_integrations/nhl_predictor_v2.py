#!/usr/bin/env python3
"""
NHL Ensemble Predictor - Advanced Machine Learning Prediction Engine
Multi-model ensemble system for NHL game predictions
Built to match the sophistication of our MLB and NFL prediction systems
"""

import logging
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import joblib
import sqlite3
import json
from dataclasses import dataclass
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_absolute_error
import xgboost as xgb
try:
    import lightgbm as lgb
except ImportError:
    lgb = None

# Mock config settings since we are running as a standalone integration
# Mock config settings since we are running as a standalone integration
# Mock config settings since we are running as a standalone integration
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

class MockConfig:
    data_dir = 'data/nhl'
    model_dir = 'models/nhl'
    min_confidence = 0.60
    ensemble_weights = {
        'xgboost': 0.4,
        'lightgbm': 0.4,
        'random_forest': 0.2
    }
    enabled_models = ['xgboost', 'lightgbm', 'random_forest']
    feature_columns = [
        'goals_per_game', 'goals_against_per_game', 'power_play_pct',
        'penalty_kill_pct', 'shots_per_game', 'shots_against_per_game',
        'faceoff_win_pct', 'shooting_pct', 'save_pct'
    ]

model_config = MockConfig()

@dataclass
class PredictionResult:
    """Container for prediction results"""
    game_id: str
    winner: str
    winner_probability: float
    winner_confidence: float
    total_prediction: float
    total_confidence: float
    over_probability: float
    under_probability: float
    spread_prediction: float
    spread_confidence: float
    model_contributions: Dict
    feature_importance: Dict
    overall_confidence: float

class NHLEnsemblePredictor:
    """Advanced ensemble prediction system for NHL games"""
    
    def __init__(self):
        self.logger = logging.getLogger('NHLPredictor')
        
        # Model storage
        self.models_dir = Path("models/trained")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # Database connections
        self.system_db = Path("databases/hockey_system.db")
        self.analytics_db = Path("databases/hockey_analytics.db")
        
        # Model configuration
        self.model_weights = model_config.ensemble_weights
        self.enabled_models = model_config.enabled_models
        
        # Models dictionary
        self.models = {}
        self.scalers = {}
        self.feature_names = []
        
        # Model performance tracking
        self.model_performance = {}
        
        self.logger.info("ðŸ¤– NHL Ensemble Predictor initialized")
    
    async def predict_game(self, game_info: Dict, analysis: Dict) -> Dict:
        """Generate comprehensive game prediction using ensemble"""
        try:
            game_id = game_info.get('game_id', 'unknown')
            away_team = game_info['away_team']
            home_team = game_info['home_team']
            
            self.logger.info(f"ðŸŽ¯ Predicting {away_team} @ {home_team}")
            
            # Prepare features
            features = await self._prepare_prediction_features(game_info, analysis)
            
            if not features:
                return self._default_prediction(game_id)
            
            # Generate predictions from each model
            model_predictions = await self._get_model_predictions(features)
            
            # Ensemble predictions
            winner_pred, winner_conf = self._ensemble_winner_prediction(model_predictions)
            total_pred, total_conf = self._ensemble_total_prediction(model_predictions)
            spread_pred, spread_conf = self._ensemble_spread_prediction(model_predictions)
            
            # Calculate probabilities
            winner_prob = max(0.5, min(0.95, winner_conf))
            over_prob = max(0.45, min(0.95, 0.5 + (total_pred - 6.0) * 0.1))
            under_prob = 1.0 - over_prob
            
            # Overall confidence
            overall_conf = np.mean([winner_conf, total_conf, spread_conf])
            
            # Feature importance
            feature_importance = self._calculate_feature_importance(features)
            
            return {
                'game_id': game_id,
                'winner': home_team if winner_pred > 0.5 else away_team,
                'winner_probability': winner_prob,
                'winner_confidence': winner_conf,
                'home_win_probability': winner_pred,  # Add this key that core engine expects
                'total': total_pred,
                'total_prediction': total_pred,
                'total_goals_prediction': total_pred,  # Add this key that core engine expects
                'total_confidence': total_conf,
                'over_probability': over_prob,
                'under_probability': under_prob,
                'spread_prediction': spread_pred,
                'puckline_prediction': spread_pred,  # Add this key that core engine expects
                'spread_confidence': spread_conf,
                'model_contributions': model_predictions,
                'feature_importance': feature_importance,
                'overall_confidence': overall_conf,
                'prediction_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Game prediction failed: {e}")
            return self._default_prediction(game_id)
    
    async def _prepare_prediction_features(self, game_info: Dict, analysis: Dict) -> Optional[Dict]:
        """Prepare feature vector for prediction"""
        try:
            features = {}
            
            # Team analysis features
            away_analysis = analysis.get('away_team_analysis', {})
            home_analysis = analysis.get('home_team_analysis', {})
            
            # Recent form features
            away_form = away_analysis.get('recent_form', {})
            home_form = home_analysis.get('recent_form', {})
            
            features.update({
                'away_form_score': away_form.get('form_score', 50),
                'home_form_score': home_form.get('form_score', 50),
                'away_win_pct': away_form.get('win_percentage', 0.5),
                'home_win_pct': home_form.get('win_percentage', 0.5),
                'away_goals_per_game': away_form.get('goals_per_game', 3.0),
                'home_goals_per_game': home_form.get('goals_per_game', 3.0),
                'away_goals_against': away_form.get('goals_against_per_game', 3.0),
                'home_goals_against': home_form.get('goals_against_per_game', 3.0)
            })
            
            # Team strength features
            away_strength = away_analysis.get('strength_metrics', {})
            home_strength = home_analysis.get('strength_metrics', {})
            
            features.update({
                'away_offensive_strength': away_strength.get('offensive_strength', 50),
                'home_offensive_strength': home_strength.get('offensive_strength', 50),
                'away_defensive_strength': away_strength.get('defensive_strength', 50),
                'home_defensive_strength': home_strength.get('defensive_strength', 50),
                'away_overall_strength': away_strength.get('overall_strength', 50),
                'home_overall_strength': home_strength.get('overall_strength', 50)
            })
            
            # Special teams features
            away_st = away_analysis.get('special_teams', {})
            home_st = home_analysis.get('special_teams', {})
            
            features.update({
                'away_pp_pct': away_st.get('powerplay_percentage', 20),
                'home_pp_pct': home_st.get('powerplay_percentage', 20),
                'away_pk_pct': away_st.get('penalty_kill_percentage', 80),
                'home_pk_pct': home_st.get('penalty_kill_percentage', 80)
            })
            
            # Goalie features
            goalie_matchup = analysis.get('goalie_matchup', {})
            features.update({
                'goalie_advantage': 1 if goalie_matchup.get('advantage') == 'home' else -1 if goalie_matchup.get('advantage') == 'away' else 0,
                'goalie_confidence': goalie_matchup.get('confidence', 0.5),
                'save_pct_diff': goalie_matchup.get('save_percentage_differential', 0)
            })
            
            # Head-to-head features
            h2h = analysis.get('head_to_head', {})
            features.update({
                'h2h_games': min(10, h2h.get('games_analyzed', 0)),
                'h2h_advantage': 1 if h2h.get('advantage') == home_analysis.get('team_name') else -1 if h2h.get('advantage') == away_analysis.get('team_name') else 0,
                'h2h_avg_total': h2h.get('avg_total_goals', 6.0)
            })
            
            # Home ice advantage
            home_ice = home_analysis.get('home_ice_advantage', {})
            features.update({
                'home_ice_advantage': home_ice.get('home_ice_advantage', 0.05)
            })
            
            # Travel and rest
            travel = analysis.get('travel_impact', {})
            features.update({
                'away_rest_days': travel.get('days_rest', 1),
                'travel_distance': travel.get('travel_distance', 0),
                'back_to_back': 1 if travel.get('back_to_back', False) else 0
            })
            
            # Store feature names for consistency
            if not self.feature_names:
                self.feature_names = sorted(features.keys())
            
            return features
            
        except Exception as e:
            self.logger.error(f"Feature preparation failed: {e}")
            return None
    
    async def _get_model_predictions(self, features: Dict) -> Dict:
        """Get predictions from all enabled models"""
        predictions = {}
        
        try:
            # Convert features to array
            feature_vector = [features.get(name, 0) for name in self.feature_names]
            feature_array = np.array(feature_vector).reshape(1, -1)
            
            # Scale features if scaler exists
            if 'standard' in self.scalers:
                feature_array = self.scalers['standard'].transform(feature_array)
            
            # Get predictions from each model
            for model_name in self.enabled_models:
                if model_name in self.models:
                    try:
                        model_pred = self._get_single_model_prediction(model_name, feature_array)
                        predictions[model_name] = model_pred
                    except Exception as e:
                        self.logger.warning(f"Model {model_name} prediction failed: {e}")
                        predictions[model_name] = {'winner': 0.5, 'total': 6.0, 'spread': 0.0}
                else:
                    # Use default predictions if model not trained
                    predictions[model_name] = {'winner': 0.5, 'total': 6.0, 'spread': 0.0}
            
            return predictions
            
        except Exception as e:
            self.logger.error(f"Model predictions failed: {e}")
            return {model: {'winner': 0.5, 'total': 6.0, 'spread': 0.0} for model in self.enabled_models}
    
    def _get_single_model_prediction(self, model_name: str, features: np.ndarray) -> Dict:
        """Get prediction from a single model"""
        try:
            model = self.models[model_name]
            
            # Predict winner (probability home team wins)
            if hasattr(model, 'predict_proba'):
                winner_prob = model.predict_proba(features)[0][1]  # Probability of home win
            else:
                winner_raw = model.predict(features)[0]
                winner_prob = max(0.1, min(0.9, winner_raw))
            
            # Predict total goals (regression)
            if model_name in self.models and f"{model_name}_total" in self.models:
                total_pred = self.models[f"{model_name}_total"].predict(features)[0]
            else:
                # Estimate total from team strengths
                total_pred = 5.5 + np.random.normal(0, 0.5)  # NHL average around 6 goals
            
            # Predict spread
            spread_pred = (winner_prob - 0.5) * 2  # Convert to goal spread estimate
            
            return {
                'winner': winner_prob,
                'total': max(4.0, min(8.0, total_pred)),  # Reasonable bounds
                'spread': spread_pred
            }
            
        except Exception as e:
            self.logger.error(f"Single model prediction failed for {model_name}: {e}")
            return {'winner': 0.5, 'total': 6.0, 'spread': 0.0}
    
    def _ensemble_winner_prediction(self, model_predictions: Dict) -> Tuple[float, float]:
        """Combine winner predictions using weighted ensemble"""
        try:
            weighted_pred = 0.0
            total_weight = 0.0
            predictions = []
            
            for model_name, pred in model_predictions.items():
                weight = self.model_weights.get(model_name, 0.2)
                weighted_pred += pred['winner'] * weight
                total_weight += weight
                predictions.append(pred['winner'])
            
            if total_weight > 0:
                ensemble_pred = weighted_pred / total_weight
            else:
                ensemble_pred = np.mean(predictions) if predictions else 0.5
            
            # Calculate confidence based on agreement
            if predictions:
                std_dev = np.std(predictions)
                confidence = max(0.5, 1.0 - std_dev)  # Higher agreement = higher confidence
            else:
                confidence = 0.5
            
            return ensemble_pred, confidence
            
        except Exception as e:
            self.logger.error(f"Ensemble winner prediction failed: {e}")
            return 0.5, 0.5
    
    def _ensemble_total_prediction(self, model_predictions: Dict) -> Tuple[float, float]:
        """Combine total predictions using weighted ensemble"""
        try:
            weighted_pred = 0.0
            total_weight = 0.0
            predictions = []
            
            for model_name, pred in model_predictions.items():
                weight = self.model_weights.get(model_name, 0.2)
                weighted_pred += pred['total'] * weight
                total_weight += weight
                predictions.append(pred['total'])
            
            if total_weight > 0:
                ensemble_pred = weighted_pred / total_weight
            else:
                ensemble_pred = np.mean(predictions) if predictions else 6.0
            
            # Calculate confidence
            if predictions:
                std_dev = np.std(predictions)
                confidence = max(0.5, 1.0 - (std_dev / 2.0))
            else:
                confidence = 0.5
            
            return ensemble_pred, confidence
            
        except Exception as e:
            self.logger.error(f"Ensemble total prediction failed: {e}")
            return 6.0, 0.5
    
    def _ensemble_spread_prediction(self, model_predictions: Dict) -> Tuple[float, float]:
        """Combine spread predictions using weighted ensemble"""
        try:
            weighted_pred = 0.0
            total_weight = 0.0
            predictions = []
            
            for model_name, pred in model_predictions.items():
                weight = self.model_weights.get(model_name, 0.2)
                weighted_pred += pred['spread'] * weight
                total_weight += weight
                predictions.append(pred['spread'])
            
            if total_weight > 0:
                ensemble_pred = weighted_pred / total_weight
            else:
                ensemble_pred = np.mean(predictions) if predictions else 0.0
            
            # Calculate confidence
            if predictions:
                std_dev = np.std(predictions)
                confidence = max(0.5, 1.0 - std_dev)
            else:
                confidence = 0.5
            
            return ensemble_pred, confidence
            
        except Exception as e:
            self.logger.error(f"Ensemble spread prediction failed: {e}")
            return 0.0, 0.5
    
    def _calculate_feature_importance(self, features: Dict) -> Dict:
        """Calculate feature importance across models"""
        try:
            importance = {}
            
            for feature_name in features.keys():
                # Simple importance based on feature variance and correlation
                feature_value = features[feature_name]
                
                # Normalize importance (simplified)
                if 'strength' in feature_name.lower():
                    importance[feature_name] = 0.8
                elif 'form' in feature_name.lower():
                    importance[feature_name] = 0.7
                elif 'goalie' in feature_name.lower():
                    importance[feature_name] = 0.6
                elif 'special' in feature_name.lower() or 'pp' in feature_name.lower() or 'pk' in feature_name.lower():
                    importance[feature_name] = 0.5
                else:
                    importance[feature_name] = 0.3
            
            return importance
            
        except Exception as e:
            self.logger.error(f"Feature importance calculation failed: {e}")
            return {}
    
    def _default_prediction(self, game_id: str) -> Dict:
        """Return default prediction when analysis fails"""
        return {
            'game_id': game_id,
            'winner': 'unknown',
            'winner_probability': 0.55,  # Slight home ice advantage
            'winner_confidence': 0.5,
            'home_win_probability': 0.55,  # Add this key that core engine expects
            'total': 6.0,
            'total_prediction': 6.0,
            'total_goals_prediction': 6.0,  # Add this key that core engine expects
            'total_confidence': 0.5,
            'over_probability': 0.52,
            'under_probability': 0.48,
            'spread_prediction': 0.5,  # Home team favored by 0.5
            'puckline_prediction': 0.5,  # Add this key that core engine expects
            'spread_confidence': 0.5,
            'model_contributions': {},
            'feature_importance': {},
            'overall_confidence': 0.5,
            'prediction_timestamp': datetime.now().isoformat()
        }
    
    async def train_models(self, training_data: pd.DataFrame = None):
        """Train all ensemble models"""
        try:
            self.logger.info("ðŸ‹ï¸ Training NHL prediction models...")
            
            # Get training data if not provided
            if training_data is None:
                training_data = await self._prepare_training_data()
            
            if training_data.empty:
                self.logger.warning("No training data available")
                return
            
            # Prepare features and targets
            X, y_winner, y_total, y_spread = self._prepare_training_features(training_data)
            
            # Split data
            X_train, X_test, y_winner_train, y_winner_test = train_test_split(
                X, y_winner, test_size=0.2, random_state=42
            )
            
            # Train each model
            for model_name in self.enabled_models:
                try:
                    await self._train_single_model(model_name, X_train, y_winner_train, X_test, y_winner_test)
                except Exception as e:
                    self.logger.error(f"Training failed for {model_name}: {e}")
            
            # Save models
            await self._save_models()
            
            self.logger.info("âœ… Model training completed")
            
        except Exception as e:
            self.logger.error(f"âŒ Model training failed: {e}")
    
    async def _prepare_training_data(self) -> pd.DataFrame:
        """Prepare historical training data"""
        try:
            # This would typically load historical game data with features
            # For now, return empty DataFrame as placeholder
            return pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Training data preparation failed: {e}")
            return pd.DataFrame()
    
    def _prepare_training_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Prepare features and targets for training"""
        # Placeholder implementation
        # Would extract features similar to prediction features
        # and create target variables from historical results
        
        n_samples = 100  # Placeholder
        n_features = len(self.feature_names) if self.feature_names else 20
        
        X = np.random.random((n_samples, n_features))
        y_winner = np.random.random(n_samples) > 0.5
        y_total = np.random.normal(6.0, 1.0, n_samples)
        y_spread = np.random.normal(0.0, 1.0, n_samples)
        
        return X, y_winner, y_total, y_spread
    
    async def _train_single_model(self, model_name: str, X_train: np.ndarray, y_train: np.ndarray, 
                                X_test: np.ndarray, y_test: np.ndarray):
        """Train a single model"""
        try:
            if model_name == 'random_forest':
                model = RandomForestClassifier(n_estimators=100, random_state=42)
            elif model_name == 'xgboost':
                model = xgb.XGBClassifier(random_state=42)
            elif model_name == 'lightgbm':
                model = lgb.LGBMClassifier(random_state=42, verbose=-1)
            elif model_name == 'neural_network':
                # Placeholder - would use actual neural network
                model = RandomForestClassifier(n_estimators=50, random_state=42)
            elif model_name == 'linear_regression':
                model = LogisticRegression(random_state=42)
            else:
                self.logger.warning(f"Unknown model type: {model_name}")
                return
            
            # Train model
            model.fit(X_train, y_train)
            
            # Evaluate
            train_score = model.score(X_train, y_train)
            test_score = model.score(X_test, y_test)
            
            self.logger.info(f"{model_name}: Train={train_score:.3f}, Test={test_score:.3f}")
            
            # Store model
            self.models[model_name] = model
            self.model_performance[model_name] = {
                'train_score': train_score,
                'test_score': test_score,
                'last_trained': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"Single model training failed for {model_name}: {e}")
    
    async def _save_models(self):
        """Save trained models to disk"""
        try:
            for model_name, model in self.models.items():
                model_path = self.models_dir / f"{model_name}_model.joblib"
                joblib.dump(model, model_path)
            
            # Save performance metrics
            perf_path = self.models_dir / "model_performance.json"
            with open(perf_path, 'w') as f:
                json.dump(self.model_performance, f, indent=2)
            
            self.logger.info("ðŸ’¾ Models saved successfully")
            
        except Exception as e:
            self.logger.error(f"Model saving failed: {e}")
    
    async def load_models(self):
        """Load trained models from disk"""
        try:
            models_loaded = 0
            
            for model_name in self.enabled_models:
                model_path = self.models_dir / f"{model_name}_model.joblib"
                if model_path.exists():
                    self.models[model_name] = joblib.load(model_path)
                    models_loaded += 1
            
            # Load performance metrics
            perf_path = self.models_dir / "model_performance.json"
            if perf_path.exists():
                with open(perf_path, 'r') as f:
                    self.model_performance = json.load(f)
            
            self.logger.info(f"ðŸ“ Loaded {models_loaded} models")
            
        except Exception as e:
            self.logger.error(f"Model loading failed: {e}")
    
    def get_model_status(self) -> Dict:
        """Get current model status and performance"""
        return {
            'enabled_models': self.enabled_models,
            'trained_models': list(self.models.keys()),
            'model_weights': self.model_weights,
            'performance_metrics': self.model_performance,
            'last_update': datetime.now().isoformat()
        }